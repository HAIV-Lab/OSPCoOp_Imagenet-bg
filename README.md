<div align="center">
  <h1>Overcoming Shortcut Learning in Vision-Language Models for Robust Out-of-Distribution Detection</h1>


  <a href="#">
  <img src="https://img.shields.io/badge/%F0%9F%93%96-CVPR_2025-red.svg?style=flat-square" alt="CVPR 2025">
</a>
  <a href='#'>
      <img src='https://img.shields.io/badge/Project-Page-green?style=flat&logo=googlechrome&logoColor=green' alt='Project page'>
    </a>
<a href="#">
    <img alt="Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg" width="100">
    <span style="margin-left: 8px; font-weight: bold;">Data</span>
</a>
</div>

<div align="center">
  <h2>
    <a href="#">Zhuo Xu</a><sup>1</sup>, 
    <a href="#">Xiang Xiang</a><sup>1,2</sup>, 
    <a href="#">Yifan Liang</a><sup>1</sup>
  </h2>
  
  <p>
    <sup>1</sup> National Key Lab of Multi-Spectral Information Intelligent Processing Technology<br>
    School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, China<br><br>
    <sup>2</sup> Peng Cheng National Laboratory, Shenzhen, China
  </p>
</div>


The code repository for "Overcoming Shortcut Problem in VLM for Robust Out-of-Distribution Detection" in PyTorch. 

</div>

## :rocket: News
[05/2025] ðŸŽ‰  We have released the code and checkpoints for training and evluation.

[04/2025] ðŸŽ‰  Our paper has been selected as a Highlight paper.

[03/2025]ðŸŽ‰  We have released the proposed [ImageNet-Bg](https://www.kaggle.com/datasets/xiangexiang/imagenet-bg-ospcoop-cvpr2025) along with the code for OOD generation.

[02/2025]ðŸŽ‰  Our paper has been accepted by CVPR2025.

## :eyes: Table of Contents
1. [The proposed ImageNet-Bg](#the-proposed-imagenet-bg)
2. [Dataset Preparation](#dataset-preparation)
3. [Train and Evaluate OSPCoOp](#train-and-evaluate-ospcoop)
4. [Acknowledgements](#acknowledgements)
5. [Citation](#citation)


## :sparkles:The proposed ImageNet-Bg

To further test the robustness of the model against background interference, we propose an ImageNet background interference test set, ImageNet-Bg, based on the ImageNet validation set with 48,285 images. All images in this dataset are generated by removing ID-relevant regions from samples in the ImageNet validation set. We filter the images to obtain the ImageNet-Bg(S) test set, which contains purer background information with 24,863 images.  

<img src="Figs/ImageNet-bg.png" width="96%">

## :wrench: Dataset Preparation

- ID Datasets: ImageNet-1K, The ImageNet-1k dataset (ILSVRC-2012) can be downloaded [here](https://image-net.org/challenges/LSVRC/2012/index.php#).
 
- OOD Datasets: iNaturalist, SUN, Places, and Texture. Please follow the instruction from [MOS](https://github.com/deeplearning-wisc/large_scale_ood#out-of-distribution-dataset).

- Background Interference OOD Datasets: ImageNet-Bg, ImageNet-Bg(S).

After downloading the datasets, update the data root path in `./OSPCoOp/root_config.py` to point to your local dataset location.

Please run the following command to sample few-shot training data for further Pseudo-OOD Generation.
```
CUDA_VISIBLE_DEVICES=2 python train.py  --trainer OSPCoOp --shots 16 --few_shot_sampler True
```

### Pseudo-OOD Generation

For quick start, we have provided our generated Pseudo-OOD data, which can be downloaded here.

#### Image Masking
Please follow these steps:

Step1: Please install [Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything).

Step2: Copy the ./GroundingSAM/masking.py and ./GroundingSAM/classnames.py file into your Grounded-Segment-Anything project directory.

Step3: Run the following command:
```
python masking.py --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py   --grounded_checkpoint groundingdino_swint_ogc.pth   --sam_checkpoint sam_vit_h_4b8939.pth   --output_dir "output_dir"   --box_threshold 0.3   --text_threshold 0.25    --device "cuda"
```
Note: Set "output_dir" to your image directory, and place the images to be processed in the "./output_dir/raw" directory.
#### Image Inpainting
Please follow these steps and set the output_dir to ./:

Step1: Please install [Inpaint-Anything](https://github.com/geekyutao/Inpaint-Anything).

Step2: Copy the ./Inpainting/inpainting.py, ./Inpainting/Texture_inpainting.py and ./GroundingSAM/classnames.py file into your Inpaint-Anything project directory.

Step3: Run the following command to generate the inpainting images:
```
python inpainting.py --output_dir output    --lama_ckpt ./pretrained_models/big-lama  --min=0.0  --max=0.25
```

Step4: Run the following command to compute the score of inpainting images:
```
python get_clip_score.py  --output_dir output
```
Step5: To generate the texture OOD aug data, please run the following command:
```
python Texture_inpainting.py  --output_dir output    --lama_ckpt ./pretrained_models/big-lama  --min=0.0  --max=0.25
```

## :computer: Train and Evaluate OSPCoOp
Our experiments are conducted with Python 3.9.18 and Pytorch 2.1.0. Followed [LoCoOp](https://github.com/AtsuMiyai/LoCoOp) and [CoOp](https://github.com/KaiyangZhou/CoOp), the training code is built on top of the awedome toolbox [Dassl](https://github.com/KaiyangZhou/Dassl.pytorch). So, you need to install `dassl` first. Then, run `pip install -r requirements.txt` to install additional packages. Please note that a new environment should be created, which is different from the aforementioned Pseudo-OOD Generation. 
### Training
After preparing Pseudo-OOD data, please run the following command to train OSPCoOp:
```
CUDA_VISIBLE_DEVICES=0 python train.py   --loss1 1.5 --loss2 0.5 --trainer OSPCoOp --shots 16 --output_dir ./runs/16shots 
```

### OOD Detection Evaluation
For quick start, we share our 16-shot OSPCoOp checkpoint, please download them via the url.

To evaluate iNaturalist, SUN, Places, and Texture OOD data, please run the following command.
```
CUDA_VISIBLE_DEVICES=0 python train.py  --trainer OSPCoOp --shots 16 --eval_only True --model-dir ./runs/16shots 
```
To evaluate ImageNet-Bg, please run the following command.
```
CUDA_VISIBLE_DEVICES=0 python train.py  --trainer OSPCoOp --shots 16 --eval_only True --model-dir ./runs/16shots  
```

## :beers:Acknowledgement
This work is based on the following repositories: [Grounded-SAM](https://arxiv.org/abs/2401.14159), [LoCoOp](https://arxiv.org/abs/2306.01293), [Inpaint Anything](https://arxiv.org/abs/2304.06790), [LaMa](https://arxiv.org/abs/2109.07161). Thanks to their excellent work!!!


## :books: Citaiton

If you find our work interesting or use our methods, please consider citing:

```

```
